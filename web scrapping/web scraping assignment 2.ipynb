{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97f34c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\muham\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59b0142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required drivers\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a10652",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "\n",
    "1.First get the webpage https://www.naukri.com/\n",
    "2.Enter “Data Analyst” in “Skill, Designations, Companies” field   and enter “Bangalore” in “enter thelocation” field. \n",
    "3.Then click the search button. \n",
    "4.Then scrape the data for the first 10 jobs results you get. \n",
    "5.Finally create a dataframe of the scraped data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f5f3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a3ea1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.naukri.com\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0617c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Data Analyst\" in \"Skill, Designation, Companies\" field\n",
    "designation=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/input')\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "#entering \"Bangalore\" in \"enter the location\" field\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[6]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18c992e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping job-title from the webpage\n",
    "job_title=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')[0:10]:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "# Scrapping job location from the webpage\n",
    "job_location=[]\n",
    "for i in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span')[0:10]:\n",
    "    job_location.append(i.text)\n",
    "\n",
    "# Scrapping Company Name from the webpage\n",
    "company_name=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[0:10]:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "# Scrapping Experience from the webpage\n",
    "experience_required=[]\n",
    "for i in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')[0:10]:\n",
    "    experience_required.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "691d64ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# converting scrapped data into DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob_Title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mjob_title\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob_Location\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mjob_location\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCompany_Name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExperience_required\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mexperience_required\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# converting scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Job_Title':job_title,'Job_Location':job_location,'Company_Name':company_name,'Experience_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7575733",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1.First get the webpage https://www.naukri.com/\n",
    "2.Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3.Then click the search button.\n",
    "4.Then scrape the data for the first 10 jobs results you get.\n",
    "5.Finally create a dataframe of the scraped data.\n",
    "\n",
    "Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60377088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "\n",
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe23927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.naukri.com\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aaa72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Data Scientist\" in \"Skill, Designation, Companies\" field\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e952cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Bangalore\" in \"enter the location\" field\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d44692b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the Search button\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[6]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8a0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping job-title from the webpage\n",
    "job_title=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')[0:10]:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "# Scrapping job location from the webpage\n",
    "job_location=[]\n",
    "for j in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span')[0:10]:\n",
    "    job_location.append(j.text)\n",
    "\n",
    "# Scrapping Company Name from the webpage\n",
    "company_name=[]\n",
    "for k in driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[0:10]:\n",
    "    company_name.append(k.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "708b29bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Job_Title, Job_Location, Company_Name]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Job_Title':job_title,'Job_Location':job_location,'Company_Name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2d489",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1.first get the webpage https://www.naukri.com/\n",
    "2.Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3.Then click the search button.\n",
    "4.Then apply the location filter and salary filter by checking the respective boxes\n",
    "5.Then scrape the data for the first 10 jobs results you get.\n",
    "6.Finally create a dataframe of the scraped data.\n",
    "\n",
    "Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "466a7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e03865ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting web-page https://www.naukri.com/\n",
    "driver.get('https://www.naukri.com/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07c83b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Data Scientist\" in \"Skill, Designation, Companies\" field\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29025bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying location filter and selecting \"Delhi/NCR\" otion\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div/section[2]/div[2]/article[1]/div[1]/ul/li[3]/span')\n",
    "location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f27b0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying Salary filter and selecting \"3-6 lakhs\" otion\n",
    "salary=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div/section[2]/div[2]/article[1]/div[1]/ul/li[2]/span[1]')\n",
    "salary.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b48214b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping job-title from the webpage\n",
    "jtitle=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')[0:10]:\n",
    "    jtitle.append(i.text)\n",
    "    \n",
    "# Scrapping job-location from the webpage\n",
    "jloc=[]\n",
    "for j in driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span')[0:10]:\n",
    "    jloc.append(j.text)\n",
    "\n",
    "# Scrapping Company name from the webpage\n",
    "comp=[]\n",
    "for k in driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[0:10]:\n",
    "    comp.append(k.text)\n",
    "    \n",
    "# Scrapping Experience required from the webpage\n",
    "exp=[]\n",
    "for l in  driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')[0:10]:\n",
    "    exp.append(l.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d429a75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# converting scrapped data into DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob_Title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mjtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob_Location\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mjloc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCompany_Name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcomp\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExperience required\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# converting scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Job_Title':jtitle,'Job_Location':jloc,'Company_Name':comp,'Experience required':exp})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c6cc8",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1.Brand\n",
    "2.Product Description\n",
    "3.Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    "1.Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2.Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3.After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4.After scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it.\n",
    "5.Now scrape data from this page as usual\n",
    "6.Repeat this until you get data for 100 sunglasses.\n",
    "Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e21134ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd20e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.flipkart.com/\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7246b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the log-in page window\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button')\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b63da2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"sunglasses\" in \"search for products\" field\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "product.send_keys('Sunglasses')\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2606ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Brand detail from the webpage\n",
    "brand=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]'):\n",
    "    brand.append(i.text)\n",
    "\n",
    "# Scrapping Product Description from the webpage\n",
    "pro1=[]\n",
    "for j in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]'):\n",
    "    pro1.append(j.text)\n",
    "    \n",
    "pro2=[]\n",
    "for j in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]'):\n",
    "    pro2.append(j.text)\n",
    "    \n",
    "pro_des=pro1+pro2\n",
    "\n",
    "# Scrapping Price from the webpage\n",
    "price=[]\n",
    "for k in driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]'):\n",
    "    price.append(k.text)\n",
    "\n",
    "# Scrapping Discount from the webpage\n",
    "dis=[]\n",
    "for l in driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]'):\n",
    "    dis.append(l.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8dd6085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Oval Sunglasses (45)</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye Sunglasses (46)</td>\n",
       "      <td>₹149</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>37% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PC STAR</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹282</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>Riding Glasses, UV Protection Retro Square Sun...</td>\n",
       "      <td>₹229</td>\n",
       "      <td>90% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RED MONK</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹270</td>\n",
       "      <td>90% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (53)</td>\n",
       "      <td>₹174</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (52)</td>\n",
       "      <td>₹599</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹149</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (59)</td>\n",
       "      <td>₹179</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>₹274</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (53)</td>\n",
       "      <td>₹149</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>₹179</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CRYSTAL CART</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹329</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>Night Vision, UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹182</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SHAAH COLLECTIONS</td>\n",
       "      <td>Polarized, UV Protection, Gradient, Riding Gla...</td>\n",
       "      <td>₹129</td>\n",
       "      <td>92% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (52)</td>\n",
       "      <td>₹259</td>\n",
       "      <td>90% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Rectangular...</td>\n",
       "      <td>₹398</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹579</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection, Polarized Rectangular Sunglasse...</td>\n",
       "      <td>₹244</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hayden haiza</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹319</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹649</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (55)</td>\n",
       "      <td>₹257</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (54)</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Resist</td>\n",
       "      <td>Others Wayfarer, Retro Square Sunglasses (50)</td>\n",
       "      <td>₹1,399</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>Polarized Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹579</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹999</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PC STAR</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹229</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>UV Protection, Riding Glasses Rectangular Sung...</td>\n",
       "      <td>₹129</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>EYELLUSION</td>\n",
       "      <td>Riding Glasses, UV Protection, Night Vision Sp...</td>\n",
       "      <td>₹281</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Night Vision, Polarized, UV Protection, Riding...</td>\n",
       "      <td>₹579</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>hayden haiza</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹291</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>Polarized, Riding Glasses Sports, Wrap-around ...</td>\n",
       "      <td>₹497</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹268</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Rich Club</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (54)</td>\n",
       "      <td>₹258</td>\n",
       "      <td>63% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ARICKS</td>\n",
       "      <td>UV Protection, Polarized Rectangular Sunglasse...</td>\n",
       "      <td>₹259</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Brand                                Product Description  \\\n",
       "0            Fastrack                 UV Protection Oval Sunglasses (45)   \n",
       "1           Elligator              UV Protection Cat-eye Sunglasses (46)   \n",
       "2            Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "3             PC STAR  UV Protection Cat-eye, Retro Square, Oval, Rou...   \n",
       "4           Elligator   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "5           Elligator  Riding Glasses, UV Protection Retro Square Sun...   \n",
       "6          PHENOMENAL  UV Protection Cat-eye, Retro Square, Oval, Rou...   \n",
       "7            RED MONK                UV Protection Round Sunglasses (54)   \n",
       "8          LIZA ANGEL         UV Protection Retro Square Sunglasses (53)   \n",
       "9       VINCENT CHASE          UV Protection Rectangular Sunglasses (52)   \n",
       "10               SRPM  Riding Glasses, Night Vision Spectacle Sunglas...   \n",
       "11          Elligator             UV Protection Wayfarer Sunglasses (59)   \n",
       "12             PIRASO             UV Protection Wayfarer Sunglasses (50)   \n",
       "13          Elligator             UV Protection Wayfarer Sunglasses (53)   \n",
       "14               SRPM              UV Protection Aviator Sunglasses (58)   \n",
       "15       CRYSTAL CART                UV Protection Round Sunglasses (53)   \n",
       "16             PIRASO  Night Vision, UV Protection Round Sunglasses (54)   \n",
       "17  SHAAH COLLECTIONS  Polarized, UV Protection, Gradient, Riding Gla...   \n",
       "18          New Specs          UV Protection Rectangular Sunglasses (52)   \n",
       "19     ROZZETTA CRAFT  UV Protection, Polarized, Mirrored Rectangular...   \n",
       "20         LIZA ANGEL   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "21           Fastrack  UV Protection, Gradient Rectangular Sunglasses...   \n",
       "22             PIRASO  UV Protection, Polarized Rectangular Sunglasse...   \n",
       "23       hayden haiza  Gradient, UV Protection Wayfarer Sunglasses (F...   \n",
       "24     ROZZETTA CRAFT              UV Protection Aviator Sunglasses (54)   \n",
       "25             PIRASO              UV Protection Aviator Sunglasses (55)   \n",
       "26               SRPM           UV Protection Clubmaster Sunglasses (54)   \n",
       "27             Resist      Others Wayfarer, Retro Square Sunglasses (50)   \n",
       "28             SUNBEE          Polarized Wayfarer Sunglasses (Free Size)   \n",
       "29           Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "30      VINCENT CHASE       UV Protection Aviator Sunglasses (Free Size)   \n",
       "31            PC STAR  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "32         LIZA ANGEL  UV Protection, Riding Glasses Rectangular Sung...   \n",
       "33         EYELLUSION  Riding Glasses, UV Protection, Night Vision Sp...   \n",
       "34           Fastrack  Night Vision, Polarized, UV Protection, Riding...   \n",
       "35       hayden haiza      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "36     ROZZETTA CRAFT  Polarized, Riding Glasses Sports, Wrap-around ...   \n",
       "37             PIRASO       UV Protection Aviator Sunglasses (Free Size)   \n",
       "38          Rich Club         UV Protection Retro Square Sunglasses (54)   \n",
       "39             ARICKS  UV Protection, Polarized Rectangular Sunglasse...   \n",
       "\n",
       "     Price Discount  \n",
       "0     ₹599  40% off  \n",
       "1     ₹149  75% off  \n",
       "2     ₹499  37% off  \n",
       "3     ₹282  71% off  \n",
       "4     ₹129  87% off  \n",
       "5     ₹229  90% off  \n",
       "6     ₹129  87% off  \n",
       "7     ₹270  90% off  \n",
       "8     ₹174  82% off  \n",
       "9     ₹599  70% off  \n",
       "10    ₹149  88% off  \n",
       "11    ₹179  88% off  \n",
       "12    ₹274  89% off  \n",
       "13    ₹149  85% off  \n",
       "14    ₹179  82% off  \n",
       "15    ₹329  78% off  \n",
       "16    ₹182  88% off  \n",
       "17    ₹129  92% off  \n",
       "18    ₹259  90% off  \n",
       "19    ₹398  80% off  \n",
       "20    ₹129  87% off  \n",
       "21    ₹579  35% off  \n",
       "22    ₹244  84% off  \n",
       "23    ₹319  77% off  \n",
       "24    ₹649  74% off  \n",
       "25    ₹257  83% off  \n",
       "26    ₹129  87% off  \n",
       "27  ₹1,399  65% off  \n",
       "28    ₹129  87% off  \n",
       "29    ₹579  35% off  \n",
       "30    ₹999  50% off  \n",
       "31    ₹229  77% off  \n",
       "32    ₹129  74% off  \n",
       "33    ₹281  87% off  \n",
       "34    ₹579  35% off  \n",
       "35    ₹291  70% off  \n",
       "36    ₹497  75% off  \n",
       "37    ₹268  83% off  \n",
       "38    ₹258  63% off  \n",
       "39    ₹259  87% off  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1st page scrapped data into DataFrame\n",
    "df=pd.DataFrame({'Brand':brand,'Product Description':pro_des,'Price':price,'Discount':dis})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78d412a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scrapping data from next pages, defining Start & End of the page\n",
    "start= 0\n",
    "end= 3\n",
    "for page in range(start,end):\n",
    "    \n",
    "# Scrapping Brand detail from the next pages\n",
    "    for j in driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]'):\n",
    "        brand.append(j.text)\n",
    "    \n",
    "# Scrapping Product Description from the next pages\n",
    "    for k in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]'):\n",
    "        pro1.append(k.text)\n",
    "        \n",
    "    for l in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]'):\n",
    "        pro2.append(l.text)\n",
    "        \n",
    "    pro_des=pro1+pro2\n",
    "    \n",
    "# Scrapping Price from the next pages    \n",
    "    for m in driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]'):\n",
    "        price.append(m.text)\n",
    "    \n",
    "# Scrapping Discount from the next pages\n",
    "    for n in driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]'):\n",
    "        dis.append(n.text)  \n",
    "    \n",
    "    nxt_button=driver.find_elements(By.XPATH,\"//a[@class='ge-49M']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[page].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        print(\"Page not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c621c426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Oval Sunglasses (45)</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye Sunglasses (46)</td>\n",
       "      <td>₹149</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Oval Sunglasses (45)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>37% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PC STAR</td>\n",
       "      <td>UV Protection Cat-eye Sunglasses (46)</td>\n",
       "      <td>₹282</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Oval Sunglasses (45)</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>CRYSTAL CART</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (52)</td>\n",
       "      <td>₹329</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Satyam Kraft</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (53)</td>\n",
       "      <td>₹299</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>₹283</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>hayden haiza</td>\n",
       "      <td>Polarized, UV Protection, Gradient, Riding Gla...</td>\n",
       "      <td>₹319</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Brand Name                                Product Description Price  \\\n",
       "0       Fastrack                 UV Protection Oval Sunglasses (45)  ₹599   \n",
       "1      Elligator              UV Protection Cat-eye Sunglasses (46)  ₹149   \n",
       "2       Fastrack                 UV Protection Oval Sunglasses (45)  ₹499   \n",
       "3        PC STAR              UV Protection Cat-eye Sunglasses (46)  ₹282   \n",
       "4      Elligator                 UV Protection Oval Sunglasses (45)  ₹129   \n",
       "..           ...                                                ...   ...   \n",
       "95  CRYSTAL CART          UV Protection Rectangular Sunglasses (52)  ₹329   \n",
       "96  Satyam Kraft             UV Protection Wayfarer Sunglasses (53)  ₹299   \n",
       "97        SUNBEE              UV Protection Aviator Sunglasses (58)  ₹283   \n",
       "98        SUNBEE  UV Protection, Gradient Rectangular Sunglasses...  ₹129   \n",
       "99  hayden haiza  Polarized, UV Protection, Gradient, Riding Gla...  ₹319   \n",
       "\n",
       "   Discount  \n",
       "0   40% off  \n",
       "1   75% off  \n",
       "2   37% off  \n",
       "3   71% off  \n",
       "4   87% off  \n",
       "..      ...  \n",
       "95  78% off  \n",
       "96  75% off  \n",
       "97  78% off  \n",
       "98  87% off  \n",
       "99  77% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting first 100 scrapped data into DataFrame\n",
    "df1=pd.DataFrame({'Brand Name':brand,'Product Description':pro_des, 'Price':price,'Discount':dis})\n",
    "df1.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf068d",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. This task will be done in following steps:\n",
    "\n",
    "1.First get the webpage https://www.flipkart.com/\n",
    "2.Enter “iphone 11” in “Search” field .\n",
    "3.Then click the search button.\n",
    "4.As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "\n",
    "1.Rating\n",
    "2.Review summary\n",
    "3.Full review\n",
    "4.You have to scrape this data for first 100 reviews.\n",
    "Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5e60878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cf6d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.flipkart.com and directly login to product detail of iphone12 , as directed by Mentor \n",
    "driver.get('https://www.flipkart.com/apple-iphone-12-black-128-gb/product-reviews/itmf1f0a58f1ecd7?pid=MOBFWBYZK3HACR72&lid=LSTMOBFWBYZK3HACR72PX4KSA&marketplace=FLIPKART ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5499b1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Wow superb camera phone\\nVery smooth speed and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>The brand is very trustworthy and i got genuin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Awesome phone … value for money.. Happy with b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>Top class performance and battery backup too. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Guys ,this is just Beast at Every Aspect of Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The most value for money iPhone of 2021 after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Best mobile phone ever…..just go for it…..exce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Writing this review after 50 days of ‘heavy’ u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>Great iPhone, but you have to consider these o...</td>\n",
       "      <td>Pros:\\n- Same screen quality compared to 12 Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Good quality product</td>\n",
       "      <td>Well I bought this at 67k..using Amex discount...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                     Review Summary  \\\n",
       "0       5                                           Terrific   \n",
       "1       5                                   Perfect product!   \n",
       "2       5                                  Terrific purchase   \n",
       "3       5                                          Must buy!   \n",
       "4       5                              Mind-blowing purchase   \n",
       "..    ...                                                ...   \n",
       "95      5                                          Brilliant   \n",
       "96      5                                          Just wow!   \n",
       "97      5                                  Terrific purchase   \n",
       "98      4  Great iPhone, but you have to consider these o...   \n",
       "99      5                               Good quality product   \n",
       "\n",
       "                                          Full Review  \n",
       "0   Wow superb camera phone\\nVery smooth speed and...  \n",
       "1   The brand is very trustworthy and i got genuin...  \n",
       "2   Awesome phone … value for money.. Happy with b...  \n",
       "3   Top class performance and battery backup too. ...  \n",
       "4   Guys ,this is just Beast at Every Aspect of Co...  \n",
       "..                                                ...  \n",
       "95  The most value for money iPhone of 2021 after ...  \n",
       "96  Best mobile phone ever…..just go for it…..exce...  \n",
       "97  Writing this review after 50 days of ‘heavy’ u...  \n",
       "98  Pros:\\n- Same screen quality compared to 12 Pr...  \n",
       "99  Well I bought this at 67k..using Amex discount...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate1=[]\n",
    "rate2=[]\n",
    "summary=[]\n",
    "f_rev=[]\n",
    "\n",
    "# for scrapping data from different pages to get first 100 reviews, defining Start & End of the page\n",
    "start=0\n",
    "end=15\n",
    "for page in range(start,end):\n",
    "    \n",
    "# Scrapping rating from different pages \n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]'):\n",
    "        rate1.append(i.text)\n",
    "        \n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1rdVr6 _1BLPMq\"]'):\n",
    "        rate2.append(i.text)\n",
    "    rate=rate1+rate2\n",
    "\n",
    "# Scrapping Review Summary from different pages\n",
    "    for j in driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]'):\n",
    "        summary.append(j.text)\n",
    "        \n",
    "# Scrapping Full Review from different pages\n",
    "    for k in driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]'):\n",
    "        f_rev.append(k.text)\n",
    "        \n",
    "#scraping the list of reviews data from different pages\n",
    "    nxt_button=driver.find_elements(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href')) #getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "\n",
    "# converting scrapped data for first 100 reviews into DataFrame\n",
    "review=pd.DataFrame({'Rating':rate,'Review Summary':summary,'Full Review':f_rev})\n",
    "review.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d22ae1",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker:\n",
    "\n",
    "1.Brand\n",
    "2.Product Description\n",
    "3.Price\n",
    "\n",
    "Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c389e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c478d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting webpage https://www.flipkart.com/\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72e522ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the log-in page window\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button')\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f1b530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"sneakers\" in \"search for products\" field\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "product.send_keys('sneakers')\n",
    "\n",
    "#clicking the Search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "518a68c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m         driver\u001b[38;5;241m.\u001b[39mget(nxt_button[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# converting scrapped data for first 100 reviews into DataFrame\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m Sneaker_data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBrand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mbrand\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProduct Description\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprice\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDiscount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m Sneaker_data\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "#introducing list for brand, product description, price & discount\n",
    "brand=[]\n",
    "prod1=[]\n",
    "prod2=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "# for scrapping data from different pages to get first 100 reviews, defining Start & End of the page\n",
    "for page in range(0,3):\n",
    "    \n",
    "# Scrapping brand list from different pages \n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"_2gmUFU _3V8rao\"]'):\n",
    "        brand.append(i.text)\n",
    "        \n",
    "# Scrapping product description from different pages \n",
    "    for j in driver.find_elements(By.XPATH,'//div[@class=\"_2gmUFU _3V8rao\"]'):\n",
    "        prod1.append(j.text)\n",
    "        \n",
    "    for c in driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]'):\n",
    "        prod2.append(c.text)\n",
    "    prod=prod1+prod2\n",
    "    \n",
    "# Scrapping price from different pages \n",
    "    for k in driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]'):\n",
    "        price.append(k.text)\n",
    "        \n",
    "# Scrapping discount from different pages \n",
    "    for l in driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]'):\n",
    "        discount.append(l.text)\n",
    "        \n",
    "#scraping the required data from different pages\n",
    "    nxt_button=driver.find_elements(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "        \n",
    "# converting scrapped data for first 100 reviews into DataFrame\n",
    "Sneaker_data=pd.DataFrame({'Brand':brand,'Product Description':prod,'Price':price,'Discount':discount})\n",
    "Sneaker_data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703d6ab",
   "metadata": {},
   "source": [
    "# Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then \n",
    "\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "898036e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "#getting webdriver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7bf5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.amazon.in/s?k=laptop&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031&dc&ds=v1%3AKlefx2Xr1Iiju42ya%2BARx8Cr9L1R8pZ2AwEuaBU9JkM&crid=ZCHABP7C49UV&qid=1674411147&rnid=12598141031&sprefix=la%2Caps%2C1553&ref=sr_nr_p_n_feature_thirteen_browse-bin_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0712db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "ratings=[]\n",
    "price=[]\n",
    "\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    \n",
    "# Scrapping rating from different pages \n",
    "    for i in driver.find_elements(By.XPATH,''):\n",
    "        title.append(i.text)\n",
    "        \n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1rdVr6 _1BLPMq\"]'):\n",
    "        rate2.append(i.text)\n",
    "    rate=rate1+rate2\n",
    "\n",
    "# Scrapping Review Summary from different pages\n",
    "    for j in driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]'):\n",
    "        summary.append(j.text)\n",
    "        \n",
    "# Scrapping Full Review from different pages\n",
    "    for k in driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]'):\n",
    "        f_rev.append(k.text)\n",
    "        \n",
    "#scraping the list of reviews data from different pages\n",
    "    nxt_button=driver.find_elements(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href')) #getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "\n",
    "# converting scrapped data for first 100 reviews into DataFrame\n",
    "review=pd.DataFrame({'Rating':rate,'Review Summary':summary,'Full Review':f_rev})\n",
    "review.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
